{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features Demo\n",
    "\n",
    "This notebook demonstrates the new text-based features implemented in the recommendation system. These features calculate semantic relationships between products and users' purchase history, helping to create more personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from lavka_recsys.config import Config\n",
    "from lavka_recsys.experiment import Experiment, ExperimentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration\n",
    "\n",
    "We'll use the updated configuration with text features enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load from file\n",
    "config = Config.from_file('default_config.yaml')\n",
    "\n",
    "# Check which text features are enabled\n",
    "text_features = [\n",
    "    feature for feature in config.features \n",
    "    if feature in ['product_embeddings', 'category_embeddings', 'user_product_distance', \n",
    "                   'text_similarity_cluster', 'text_diversity_features']\n",
    "]\n",
    "print(f\"Enabled text features: {text_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Overview\n",
    "\n",
    "Let's review the text features we've implemented:\n",
    "\n",
    "1. **user_product_distance**: Calculates weighted similarity between target products and user's purchase/cart history\n",
    "   - `purchase_weighted_similarity`: Similarity between product and weighted purchase history\n",
    "   - `cart_weighted_similarity`: Similarity between product and weighted cart history\n",
    "   - `min_purchase_similarity`: Similarity to closest purchased product\n",
    "   - `min_cart_similarity`: Similarity to closest carted product\n",
    "\n",
    "2. **text_similarity_cluster**: Clusters products based on semantic similarity\n",
    "   - `cluster`: Which semantic cluster the product belongs to\n",
    "   - `cluster_purchase_ratio`: How often user buys from this cluster\n",
    "   - `cluster_cart_ratio`: How often user adds items from this cluster to cart\n",
    "\n",
    "3. **text_diversity_features**: Measures novelty relative to user's typical purchases\n",
    "   - `distance_from_centroid`: How different from user's typical purchases\n",
    "   - `relative_diversity`: Normalized novelty metric\n",
    "\n",
    "These features help capture semantic relationships between products and user preferences in ways that traditional collaborative filtering can't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Simple Experiment\n",
    "\n",
    "Let's run a simple experiment to see how these features perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set experiment name and output directory\n",
    "experiment_name = \"text_features_demo\"\n",
    "results_dir = f\"results/{experiment_name}\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Configure our experiment\n",
    "text_config = config.copy()\n",
    "text_config.set('experiment.type', 'single_run')\n",
    "text_config.set('experiment.use_hyperparameter_tuning', False)\n",
    "text_config.set('feature_selection.enabled', True)\n",
    "text_config.set('feature_selection.n_features', 30)  # Using more features to ensure our text features are included\n",
    "text_config.set('output.results_dir', results_dir)\n",
    "text_config.set('data.sample_fraction', 0.1)  # Use a smaller dataset for faster execution\n",
    "\n",
    "# Create experiment\n",
    "text_experiment = Experiment(experiment_name, text_config)\n",
    "\n",
    "# Setup and run experiment\n",
    "text_experiment.setup()\n",
    "results = text_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Results and Feature Importance\n",
    "\n",
    "Let's examine how important our text features are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Print metrics\n",
    "print(\"Experiment Metrics:\")\n",
    "if 'metrics' in results:\n",
    "    for metric, value in results['metrics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Check feature importance\n",
    "if 'feature_importance' in results:\n",
    "    # Get all feature importance\n",
    "    importances = results['feature_importance']\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': list(importances.keys()),\n",
    "        'importance': list(importances.values())\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_n = min(20, len(importance_df))\n",
    "    plt.barh(importance_df['feature'][:top_n], importance_df['importance'][:top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {top_n} Feature Importances')\n",
    "    plt.gca().invert_yaxis()  # Display with highest importance at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show text feature importances specifically\n",
    "    text_importance = importance_df[importance_df['feature'].str.contains('|'.join([\n",
    "        'embedding', 'text', 'distance', 'similarity', 'diversity', 'cluster'\n",
    "    ]))]\n",
    "    \n",
    "    print(\"\\nText Feature Importances:\")\n",
    "    display(text_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison Experiment Without Text Features\n",
    "\n",
    "To quantify the impact of our text features, let's run a comparison experiment without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a configuration without text features\n",
    "no_text_config = text_config.copy()\n",
    "no_text_features = [f for f in no_text_config.features if f not in text_features]\n",
    "no_text_config.set('features', no_text_features)\n",
    "no_text_config.set('output.results_dir', f\"{results_dir}/no_text\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(f\"{results_dir}/no_text\", exist_ok=True)\n",
    "\n",
    "# Create and run experiment\n",
    "no_text_experiment = Experiment(f\"{experiment_name}_no_text\", no_text_config)\n",
    "no_text_experiment.setup()\n",
    "no_text_results = no_text_experiment.run()\n",
    "\n",
    "# Print metrics\n",
    "print(\"Experiment Metrics Without Text Features:\")\n",
    "if 'metrics' in no_text_results:\n",
    "    for metric, value in no_text_results['metrics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Performance\n",
    "\n",
    "Let's visualize the performance difference between models with and without text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract metrics for comparison\n",
    "if 'metrics' in results and 'metrics' in no_text_results:\n",
    "    # Get common metrics\n",
    "    common_metrics = set(results['metrics'].keys()) & set(no_text_results['metrics'].keys())\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    metrics_to_plot = ['auc', 'precision', 'recall', 'f1', 'ndcg']\n",
    "    metrics_to_plot = [m for m in metrics_to_plot if m in common_metrics]\n",
    "    \n",
    "    if metrics_to_plot:\n",
    "        # Create comparison bar chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x = range(len(metrics_to_plot))\n",
    "        width = 0.35\n",
    "        \n",
    "        with_text_values = [results['metrics'].get(m, 0) for m in metrics_to_plot]\n",
    "        without_text_values = [no_text_results['metrics'].get(m, 0) for m in metrics_to_plot]\n",
    "        \n",
    "        plt.bar(x, with_text_values, width, label='With Text Features')\n",
    "        plt.bar([i + width for i in x], without_text_values, width, label='Without Text Features')\n",
    "        \n",
    "        plt.xlabel('Metric')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Performance Comparison: With vs. Without Text Features')\n",
    "        plt.xticks([i + width/2 for i in x], metrics_to_plot)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate percentage improvement\n",
    "        print(\"\\nPercentage Improvement with Text Features:\")\n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            with_val = with_text_values[i]\n",
    "            without_val = without_text_values[i]\n",
    "            if without_val > 0:  # Avoid division by zero\n",
    "                improvement = (with_val - without_val) / without_val * 100\n",
    "                print(f\"  {metric}: {improvement:.2f}%\")\n",
    "    else:\n",
    "        print(\"No common metrics found for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Kaggle Submission\n",
    "\n",
    "Let's create a submission file using our model with text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create submission\n",
    "submission_df = text_experiment.create_kaggle_submission()\n",
    "\n",
    "# Save submission to CSV\n",
    "submission_path = f\"{results_dir}/text_features_submission.csv\"\n",
    "submission_df.write_csv(submission_path)\n",
    "\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "print(f\"\\nSubmission Preview (first 5 rows):\")\n",
    "display(submission_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the use of three advanced text-based features:\n",
    "\n",
    "1. **user_product_distance**: Weighted similarity between products and user purchase history\n",
    "2. **text_similarity_cluster**: Product clusters based on semantic similarity  \n",
    "3. **text_diversity_features**: Novelty metrics comparing products to user's typical preferences\n",
    "\n",
    "These features capture semantic relationships that traditional collaborative filtering methods miss, especially for new or rare items. By comparing performance with and without these features, we can see their impact on recommendation quality.\n",
    "\n",
    "Potential next steps:\n",
    "- Fine-tune embedding model parameters\n",
    "- Experiment with different similarity metrics\n",
    "- Test other clustering algorithms beyond k-means\n",
    "- Combine text features with collaborative filtering in different ways"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}