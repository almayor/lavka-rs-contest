{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text Features Demo (Revised)\n",
                "\n",
                "This notebook demonstrates the text-based features implemented in the recommendation system. These features aim to capture semantic relationships between products, and between products and users' historical interactions, to create more personalized and relevant recommendations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Logger lavka_recsys (DEBUG)>"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "import polars as pl\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns # For better plot aesthetics\n",
                "from IPython.display import display\n",
                "from pathlib import Path\n",
                "\n",
                "from lavka_recsys import Config, Experiment, setup_logging\n",
                "\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "setup_logging()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Configuration and Define Text Features\n",
                "\n",
                "We'll load the default configuration and then explicitly define which text features we want to test. \n",
                "**Important Note on Embedding Dimensions:** The `num_cols` parameter in the `@FeatureFactory.register` decorators within `text_processor.py` (e.g., for `product_embeddings`, `category_embeddings`) must align with the actual number of embedding dimensions produced. This dimension is controlled by `text_processing.embedding_dimensions` in the configuration if PCA reduction is applied, or it's the raw embedding size of the pre-trained model. For this demo, we'll set it explicitly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Base configuration loaded. Intended text features for demo: ['product_embeddings', 'category_embeddings', 'user_product_similarity', 'text_similarity_cluster', 'text_diversity_features']\n",
                        "Configured embedding dimensions (after potential PCA): 20\n",
                        "Configured number of text clusters: 15\n"
                    ]
                }
            ],
            "source": [
                "# Load base configuration from file\n",
                "config = Config.load('default_config.yaml')\n",
                "\n",
                "# Define the text feature generator names as registered in text_processor.py\n",
                "ALL_TEXT_FEATURE_GENERATORS = [\n",
                "    'product_embeddings',          # Generates product_embed_X columns\n",
                "    'category_embeddings',         # Generates cat_embed_X columns\n",
                "    # 'user_product_similarity',     # Generates similarity scores (weighted, max history)\n",
                "    'text_similarity_cluster',     # Generates product_text_cluster and cluster_X_ratio columns\n",
                "    'text_diversity_features'      # Generates distance_from_centroid, relative_diversity\n",
                "]\n",
                "\n",
                "# This MUST match the num_cols in their respective @FeatureFactory.register decorators in text_processor.py\n",
                "config.set('text_processing.embedding_dimensions', 20) \n",
                "\n",
                "print(f\"Base configuration loaded. Intended text features for demo: {ALL_TEXT_FEATURE_GENERATORS}\")\n",
                "print(f\"Configured embedding dimensions (after potential PCA): {config['feature_config.text_processing.embedding_dimensions']}\")\n",
                "print(f\"Configured number of text clusters: {config['feature_config.text_processing.n_clusters']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Overview (Updated)\n",
                "\n",
                "Let's review the text features we'll be demonstrating (names reflect the revised `text_processor.py`):\n",
                "\n",
                "1.  **`product_embeddings`**: Generates dense vector representations (embeddings) for product names.\n",
                "    * Outputs columns like `product_embed_0`, `product_embed_1`, ...\n",
                "2.  **`category_embeddings`**: Generates embeddings for product category names.\n",
                "    * Outputs columns like `cat_embed_0`, `cat_embed_1`, ...\n",
                "3.  **`user_product_similarity`**: Calculates similarity scores between target products and a user's historical interactions (purchases, cart additions) based on their embeddings.\n",
                "    * `purchase_weighted_similarity`: Cosine similarity to the weighted average embedding of user's purchased items.\n",
                "    * `cart_weighted_similarity`: Cosine similarity to the weighted average embedding of user's cart items.\n",
                "    * `max_purchase_similarity_history`: Maximum cosine similarity to any single item previously purchased by the user.\n",
                "    * `max_cart_similarity_history`: Maximum cosine similarity to any single item previously added to cart by the user.\n",
                "4.  **`text_similarity_cluster`**: Clusters products based on the semantic similarity of their text embeddings (e.g., product names).\n",
                "    * `product_text_cluster` (Categorical): The ID of the semantic cluster the product belongs to.\n",
                "    * `cluster_purchase_ratio`: Ratio of a user's purchases from the target product's cluster to their total purchases.\n",
                "    * `cluster_cart_ratio`: Ratio of a user's cart additions from the target product's cluster.\n",
                "    * `cluster_view_ratio`: Ratio of a user's views from the target product's cluster.\n",
                "5.  **`text_diversity_features`**: Measures how textually different or novel a target product is compared to the user's historical interactions.\n",
                "    * `distance_from_centroid`: Euclidean distance between the target product's embedding and the centroid of the user's historically interacted items' embeddings.\n",
                "    * `relative_diversity`: The `distance_from_centroid` normalized by the user's average historical interaction diversity.\n",
                "\n",
                "These features help capture semantic relationships and user preferences that might be missed by traditional collaborative filtering or count-based features, especially for new or rare items."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Experiment with Text Features\n",
                "\n",
                "Let's configure and run an experiment that includes all our text features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running experiment WITH text features. Output directory: results/text_features_demo_with_text\n",
                        "Features to be generated: None\n",
                        "2025-05-18 00:33:27,766 - lavka_recsys.Experiment(text_features_demo_with_text_9e9c4d) - INFO - Initialized experiment: text_features_demo_with_text_9e9c4d\n",
                        "2025-05-18 00:33:27,773 - lavka_recsys.Experiment(text_features_demo_with_text_9e9c4d) - INFO - Config saved: results/text_features_demo_with_text/text_features_demo_with_text_9e9c4d_config.json\n",
                        "2025-05-18 00:33:27,774 - lavka_recsys.Experiment(text_features_demo_with_text_9e9c4d) - INFO - Setting up experiment environment...\n",
                        "2025-05-18 00:33:27,776 - lavka_recsys.DataLoader - INFO - Loading training data from ../../data/lavka/train.parquet\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2025-05-18 00:33:28,070 - lavka_recsys.DataLoader - INFO - Loading test data from ../../data/lavka/test.parquet\n",
                        "2025-05-18 00:33:28,406 - lavka_recsys.DataLoader - INFO - Holdout Split:\n",
                        "2025-05-18 00:33:28,431 - lavka_recsys.DataLoader - INFO -   train:\t2022-12-31 18:46:42 → 2024-01-03 17:31:52 (15_070_276 rows, 367 days)\n",
                        "2025-05-18 00:33:28,434 - lavka_recsys.DataLoader - INFO -   holdout:\t2024-01-03 17:56:48 → 2024-02-02 17:34:51 (1_438_338 rows, 29 days)\n",
                        "2025-05-18 00:33:28,435 - lavka_recsys.Experiment(text_features_demo_with_text_9e9c4d) - INFO - Setup complete.\n",
                        "2025-05-18 00:33:28,436 - lavka_recsys.Experiment(text_features_demo_with_text_9e9c4d) - INFO - Starting experiment run...\n",
                        "2025-05-18 00:33:28,792 - lavka_recsys.DataLoader - INFO - Validation Split:\n",
                        "2025-05-18 00:33:28,815 - lavka_recsys.DataLoader - INFO -   train_history:\t2022-12-31 18:46:42 → 2023-11-04 17:16:23 (12_082_523 rows, 307 days)\n",
                        "2025-05-18 00:33:28,819 - lavka_recsys.DataLoader - INFO -   train_target:\t2023-11-04 18:19:12 → 2023-12-04 17:25:02 (1_498_126 rows, 29 days)\n",
                        "2025-05-18 00:33:28,844 - lavka_recsys.DataLoader - INFO -   val_history:\t2022-12-31 18:46:42 → 2023-12-04 17:25:02 (13_580_649 rows, 337 days)\n",
                        "2025-05-18 00:33:28,848 - lavka_recsys.DataLoader - INFO -   val_target:\t2023-12-04 18:02:13 → 2024-01-03 17:31:52 (1_489_627 rows, 29 days)\n",
                        "2025-05-18 00:33:28,873 - lavka_recsys.CachedFeatureFactory - INFO - Generating feature batch\n",
                        "2025-05-18 00:33:28,874 - lavka_recsys.FeatureFactory - INFO - Invoking feature generators: source_type, count_purchase_user_product, count_purchase_user_store, ctr_product, cart_to_purchase_rate, purchase_view_ratio, product_embeddings, category_embeddings, user_product_similarity, text_similarity_cluster, text_diversity_features\n",
                        "2025-05-18 00:33:30,435 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Initializing TextProcessor with model: sentence-transformers - paraphrase-multilingual-MiniLM-L12-v2\n",
                        "2025-05-18 00:33:34,111 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Loaded sentence-transformers model: paraphrase-multilingual-MiniLM-L12-v2\n",
                        "2025-05-18 00:33:34,123 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Generating embeddings for unique product_id from product_name\n",
                        "2025-05-18 00:33:50,959 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Reduced embeddings from 384 to 20 dimensions using PCA.\n",
                        "2025-05-18 00:33:50,964 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Generated and cached embeddings for product_id with shape (24610, 21)\n",
                        "2025-05-18 00:33:51,089 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Generating embeddings for unique product_category from product_category\n",
                        "2025-05-18 00:33:52,953 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Reduced embeddings from 384 to 20 dimensions using PCA.\n",
                        "2025-05-18 00:33:52,968 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Generated and cached embeddings for product_category with shape (2134, 21)\n",
                        "2025-05-18 00:33:53,040 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Starting user_product_similarity generation.\n",
                        "2025-05-18 00:33:53,057 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Using cached embeddings for product_id with key product_id_sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_20\n",
                        "2025-05-18 00:33:53,119 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Created embedding lookup for 24610 products.\n",
                        "2025-05-18 00:33:55,429 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Processed purchase history for 1570 users.\n",
                        "2025-05-18 00:33:55,431 - lavka_recsys.lavka_recsys.feature_generators.text_processor - INFO - Processed cart history for 1789 users.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3525b914ee074993aec680fbc807009e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/1498126 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Set experiment name and output directory\n",
                "experiment_name_with_text = \"text_features_demo_with_text\"\n",
                "results_dir_with_text = Path(config.get('output.results_dir', 'results')) / experiment_name_with_text\n",
                "os.makedirs(results_dir_with_text, exist_ok=True)\n",
                "\n",
                "# Add text features to the list of features to be generated\n",
                "current_features = config.get('features', [])\n",
                "updated_features = list(set(current_features + ALL_TEXT_FEATURE_GENERATORS)) # Use set to avoid duplicates\n",
                "\n",
                "# Configure experiment settings for the demo\n",
                "text_exp_config = (\n",
                "    config\n",
                "    .set('features', updated_features)\n",
                "    .set('experiment.type', 'single_run')\n",
                "    .set('experiment.use_hyperparameter_tuning', False)\n",
                "    .set('output.results_dir', str(results_dir_with_text))\n",
                "    .set('data.sample_fraction', config.get('data.sample_fraction', 0.1)) \n",
                ")\n",
                "\n",
                "print(f\"Running experiment WITH text features. Output directory: {results_dir_with_text}\")\n",
                "print(f\"Features to be generated: {text_exp_config.get('features')}\")\n",
                "\n",
                "# Create and run experiment\n",
                "results_with_text = None\n",
                "try:\n",
                "    text_experiment_runner = Experiment(experiment_name_with_text, text_exp_config)\n",
                "    text_experiment_runner.setup()\n",
                "    results_with_text = text_experiment_runner.run()\n",
                "    print(\"Experiment WITH text features completed.\")\n",
                "except Exception as e:\n",
                "    print(f\"ERROR running experiment WITH text features: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyze Results and Feature Importance (With Text Features)\n",
                "\n",
                "Let's examine the metrics and see how important our text features are in the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if results_with_text and 'metrics' in results_with_text:\n",
                "    print(\"\\nExperiment Metrics (WITH Text Features):\")\n",
                "    for metric, value in results_with_text['metrics'].items():\n",
                "        print(f\"  {metric}: {value:.4f}\")\n",
                "else:\n",
                "    print(\"No metrics found for the experiment WITH text features.\")\n",
                "\n",
                "if results_with_text and 'feature_importance' in results_with_text:\n",
                "    importances = results_with_text['feature_importance']\n",
                "    if importances:\n",
                "        importance_df = pd.DataFrame({\n",
                "            'feature': list(importances.keys()),\n",
                "            'importance': list(importances.values())\n",
                "        }).sort_values('importance', ascending=False)\n",
                "        \n",
                "        plt.figure(figsize=(12, 8))\n",
                "        top_n = min(30, len(importance_df))\n",
                "        sns.barplot(x='importance', y='feature', data=importance_df.head(top_n), palette='viridis')\n",
                "        plt.title(f'Top {top_n} Feature Importances (WITH Text Features)')\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "        \n",
                "        # Define patterns to identify text-based features by their typical name components\n",
                "        text_feature_patterns = [\n",
                "            'embed', 'similarity', 'cluster', 'diversity', 'centroid', \n",
                "            'product_text_cluster' \n",
                "        ]\n",
                "        text_feature_regex = '|'.join(text_feature_patterns)\n",
                "        \n",
                "        text_importance_df = importance_df[importance_df['feature'].str.contains(text_feature_regex, case=False, na=False)]\n",
                "        \n",
                "        print(\"\\nText Feature Importances (from model with text features):\")\n",
                "        if not text_importance_df.empty:\n",
                "            display(text_importance_df)\n",
                "        else:\n",
                "            print(\"No text features found in the importance list based on patterns.\")\n",
                "            print(f\"Patterns used: {text_feature_patterns}\")\n",
                "            print(\"Available features:\", importance_df['feature'].tolist()[:20]) \n",
                "    else:\n",
                "        print(\"Feature importance data is empty.\")\n",
                "else:\n",
                "    print(\"No feature importance found for the experiment WITH text features.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Comparison Experiment Without Text Features\n",
                "\n",
                "To quantify the impact, let's run a similar experiment but explicitly exclude our text feature generators."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "experiment_name_no_text = \"text_features_demo_no_text\"\n",
                "results_dir_no_text = Path(config.get('output.results_dir', 'results')) / experiment_name_no_text\n",
                "os.makedirs(results_dir_no_text, exist_ok=True)\n",
                "\n",
                "no_text_exp_config = config.copy()\n",
                "\n",
                "# Exclude text features\n",
                "base_features = no_text_exp_config.get('features', [])\n",
                "features_without_text_generators = [f for f in base_features if f not in ALL_TEXT_FEATURE_GENERATORS]\n",
                "no_text_exp_config.set('features', features_without_text_generators)\n",
                "\n",
                "# Configure experiment settings\n",
                "no_text_exp_config.set('experiment.type', 'single_run')\n",
                "no_text_exp_config.set('experiment.use_hyperparameter_tuning', False)\n",
                "no_text_exp_config.set('feature_selection.enabled', True) \n",
                "no_text_exp_config.set('feature_selection.n_features', 50) \n",
                "no_text_exp_config.set('output.results_dir', str(results_dir_no_text))\n",
                "no_text_exp_config.set('data.sample_fraction', config.get('data.sample_fraction', 0.1))\n",
                "\n",
                "print(f\"Running experiment WITHOUT text features. Output directory: {results_dir_no_text}\")\n",
                "print(f\"Features to be generated: {no_text_exp_config.get('features')}\")\n",
                "\n",
                "# Create and run experiment\n",
                "results_no_text = None\n",
                "try:\n",
                "    no_text_experiment_runner = Experiment(experiment_name_no_text, no_text_exp_config)\n",
                "    no_text_experiment_runner.setup()\n",
                "    results_no_text = no_text_experiment_runner.run()\n",
                "    print(\"Experiment WITHOUT text features completed.\")\n",
                "except Exception as e:\n",
                "    print(f\"ERROR running experiment WITHOUT text features: {e}\")\n",
                "    import traceback\n",
                "    traceback.print_exc()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Compare Performance\n",
                "\n",
                "Let's visualize the performance difference between the model trained with text features and the one without."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if results_with_text and 'metrics' in results_with_text and results_no_text and 'metrics' in results_no_text:\n",
                "    metrics_with = results_with_text['metrics']\n",
                "    metrics_without = results_no_text['metrics']\n",
                "    \n",
                "    # Get common metrics for fair comparison\n",
                "    common_metric_keys = sorted(list(set(metrics_with.keys()) & set(metrics_without.keys())))\n",
                "    \n",
                "    if common_metric_keys:\n",
                "        metrics_to_plot_df = pd.DataFrame({\n",
                "            'Metric': common_metric_keys,\n",
                "            'With Text Features': [metrics_with.get(m, 0) for m in common_metric_keys],\n",
                "            'Without Text Features': [metrics_without.get(m, 0) for m in common_metric_keys]\n",
                "        })\n",
                "        \n",
                "        display(metrics_to_plot_df.set_index('Metric'))\n",
                "        \n",
                "        # Plotting\n",
                "        metrics_to_plot_df.set_index('Metric').plot(kind='bar', figsize=(12, 7))\n",
                "        plt.title('Performance Comparison: With vs. Without Text Features')\n",
                "        plt.ylabel('Score')\n",
                "        plt.xticks(rotation=45, ha='right')\n",
                "        plt.legend(title='Model Type')\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "        \n",
                "        print(\"\\nPercentage Improvement with Text Features:\")\n",
                "        for metric_name in common_metric_keys:\n",
                "            val_with = metrics_with.get(metric_name, 0)\n",
                "            val_without = metrics_without.get(metric_name, 0)\n",
                "            if val_without != 0:  \n",
                "                improvement = ((val_with - val_without) / abs(val_without)) * 100\n",
                "                print(f\"  {metric_name}: {improvement:.2f}%\")\n",
                "            elif val_with > 0:\n",
                "                 print(f\"  {metric_name}: N/A (baseline is 0, new score is {val_with:.4f})\")\n",
                "            else:\n",
                "                 print(f\"  {metric_name}: N/A (both scores are 0 or baseline is 0)\")\n",
                "    else:\n",
                "        print(\"No common metrics found between the two experiments for comparison.\")\n",
                "else:\n",
                "    print(\"Metrics not available for one or both experiments. Cannot compare performance.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Create Kaggle Submission (Optional)\n",
                "\n",
                "If the model with text features performs well, you can generate a submission file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if results_with_text: # Check if the experiment ran successfully\n",
                "    print(\"\\nAttempting to create Kaggle submission file from the model WITH text features...\")\n",
                "    try:\n",
                "        # The Experiment class instance is 'text_experiment_runner'\n",
                "        submission_df = text_experiment_runner.create_kaggle_submission()\n",
                "        \n",
                "        submission_path = results_dir_with_text / \"text_features_submission.csv\"\n",
                "        submission_df.write_csv(submission_path)\n",
                "        \n",
                "        print(f\"\\nSubmission file saved to: {submission_path}\")\n",
                "        print(\"Submission Preview (first 5 rows):\")\n",
                "        display(submission_df.head(5))\n",
                "    except AttributeError as e:\n",
                "        print(f\"Could not create submission. Experiment object might not be fully available or method missing: {e}\")\n",
                "    except Exception as e:\n",
                "        print(f\"An error occurred during submission file creation: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "else:\n",
                "    print(\"Skipping submission file generation as the experiment with text features did not complete successfully or yield results.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Conclusion & Next Steps\n",
                "\n",
                "In this notebook, we've demonstrated the integration and potential impact of several text-based features. By comparing model performance with and without these features, we can assess their contribution.\n",
                "\n",
                "Key takeaways:\n",
                "* Text features can capture semantic nuances that other feature types might miss.\n",
                "* The importance of individual text features can vary depending on the dataset and model.\n",
                "* Careful configuration (e.g., embedding dimensions, model choices for `TextProcessor`) is important.\n",
                "\n",
                "Potential next steps for further improvement:\n",
                "* **Hyperparameter Tuning**: Tune the parameters of the text processing (e.g., `embedding_dimensions`, `n_clusters`) and the main recommendation model (e.g., CatBoost parameters) when text features are included.\n",
                "* **Advanced Text Models**: Experiment with larger or more domain-specific pre-trained models for `TextProcessor` if computational resources allow.\n",
                "* **Different Similarity/Distance Metrics**: Explore alternatives to cosine similarity or Euclidean distance where appropriate.\n",
                "* **Interaction with Other Features**: Investigate how text features interact with other existing features in your model.\n",
                "* **Error Analysis**: Deep dive into cases where the model with text features performs significantly better or worse to understand their effect more granularly."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "RS-venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
