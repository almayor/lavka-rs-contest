{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lavka Recommender System Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the refactored recommender system with the unified experiment interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lavka_recsys.config import Config\n",
    "from lavka_recsys.experiment import Experiment, ExperimentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration\n",
    "\n",
    "You can either load configuration from a YAML file or create it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file if it exists\n",
    "if os.path.exists('default_config.yaml'):\n",
    "    config = Config.from_file('default_config.yaml')\n",
    "else:\n",
    "    # Create configuration programmatically\n",
    "    config = Config({\n",
    "        \"experiment\": {\n",
    "            \"type\": \"standard\",\n",
    "            \"use_feature_selection\": False,\n",
    "            \"use_hyperparameter_tuning\": False,\n",
    "            \"evaluation\": {\n",
    "                \"perform_kaggle_simulation\": True,\n",
    "                \"create_submission\": True\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"catboost\",\n",
    "            \"config\": {\n",
    "                \"catboost\": {\n",
    "                    \"iterations\": 300,\n",
    "                    \"learning_rate\": 0.1,\n",
    "                    \"depth\": 6,\n",
    "                    \"l2_leaf_reg\": 3.0\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"features\": [\n",
    "            \"count_purchase_user_product\",\n",
    "            \"count_purchase_user_store\",\n",
    "            \"ctr_product\",\n",
    "            \"recency_user_product\",\n",
    "            \"user_stats\",\n",
    "            \"product_stats\",\n",
    "            \"store_stats\"\n",
    "        ],\n",
    "        \"target\": \"CartUpdate_Purchase_vs_View\",\n",
    "        \"data\": {\n",
    "            \"train_path\": \"data/train.parquet\",\n",
    "            \"test_path\": \"data/test.parquet\",\n",
    "            \"sample_size\": 50000  # Use a smaller dataset for faster execution\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"split_type\": \"sliding_window\",\n",
    "            \"target_days\": 7,\n",
    "            \"step_days\": 7,\n",
    "            \"max_splits\": 5,\n",
    "            \"validation_days\": 7\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Setup Experiment\n",
    "\n",
    "Create an experiment and set it up by loading the data and initializing components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "experiment = Experiment(\"example_experiment\", config)\n",
    "\n",
    "# Setup experiment (load data, initialize components)\n",
    "experiment.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Standard Experiment\n",
    "\n",
    "Run a standard experiment with a single train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment type to STANDARD\n",
    "experiment.experiment_type = ExperimentType.STANDARD\n",
    "\n",
    "# Run experiment\n",
    "results = experiment.run()\n",
    "\n",
    "# Print metrics\n",
    "print(\"Standard Experiment Metrics:\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "top_features = sorted(results['feature_importance'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for feature, importance in top_features:\n",
    "    print(f\"  {feature}: {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full History Experiment\n",
    "\n",
    "Run a full history experiment with time-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment type to FULL_HISTORY\n",
    "experiment.experiment_type = ExperimentType.FULL_HISTORY\n",
    "\n",
    "# Run experiment (this will take longer)\n",
    "results = experiment.run()\n",
    "\n",
    "# Print metrics\n",
    "print(\"Full History Experiment Metrics:\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "top_features = sorted(results['feature_importance'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for feature, importance in top_features:\n",
    "    print(f\"  {feature}: {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kaggle Evaluation and Submission\n",
    "\n",
    "Evaluate the model on simulated Kaggle test set and create a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model and create submission\n",
    "evaluation = experiment.evaluate()\n",
    "\n",
    "# Print Kaggle simulation metrics\n",
    "if 'kaggle_simulation' in evaluation:\n",
    "    print(\"Kaggle Simulation Metrics:\")\n",
    "    for metric, value in evaluation['kaggle_simulation'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning Example\n",
    "\n",
    "Run an experiment with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration for tuning\n",
    "config.set('experiment.type', 'tuning')\n",
    "config.set('experiment.use_hyperparameter_tuning', True)\n",
    "config.set('hyperparameter_tuning.n_trials', 5)  # Low number for demonstration\n",
    "\n",
    "# Create and setup new experiment\n",
    "tuning_experiment = Experiment(\"tuning_example\", config)\n",
    "tuning_experiment.setup()\n",
    "\n",
    "# Run tuning (this will take longer)\n",
    "tuning_results = tuning_experiment.run()\n",
    "\n",
    "# Print results\n",
    "print(\"Tuning Experiment Metrics:\")\n",
    "for metric, value in tuning_results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in tuning_results['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Splitting Strategies\n",
    "\n",
    "Compare different time splitting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different time splitting configurations\n",
    "split_configs = {\n",
    "    'sliding_window': {\n",
    "        'split_type': 'sliding_window',\n",
    "        'target_days': 7,\n",
    "        'step_days': 7,\n",
    "        'max_splits': 3\n",
    "    },\n",
    "    'fixed_window': {\n",
    "        'split_type': 'fixed_window',\n",
    "        'history_days': 30,\n",
    "        'target_days': 7,\n",
    "        'step_days': 7,\n",
    "        'max_splits': 3\n",
    "    },\n",
    "    'expanding_window': {\n",
    "        'split_type': 'expanding_window',\n",
    "        'target_days': 7,\n",
    "        'step_days': 7,\n",
    "        'max_splits': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set up for comparison\n",
    "metrics = {}\n",
    "\n",
    "for split_name, split_config in split_configs.items():\n",
    "    print(f\"\\nRunning experiment with {split_name} split\")\n",
    "    \n",
    "    # Update configuration\n",
    "    for key, value in split_config.items():\n",
    "        config.set(f'training.{key}', value)\n",
    "    \n",
    "    # Create and setup experiment\n",
    "    config.set('experiment.type', 'full_history')\n",
    "    split_experiment = Experiment(f\"{split_name}_split\", config)\n",
    "    split_experiment.setup()\n",
    "    \n",
    "    # Run experiment\n",
    "    split_results = split_experiment.run()\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics[split_name] = split_results['metrics']\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"{split_name.capitalize()} Split Metrics:\")\n",
    "    for metric, value in split_results['metrics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Comparison\n",
    "\n",
    "Compare metrics across different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for AUC comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "auc_values = [metrics[split].get('auc', 0) for split in split_configs.keys()]\n",
    "ndcg_values = [metrics[split].get('ndcg@10', 0) for split in split_configs.keys()]\n",
    "\n",
    "x = range(len(split_configs))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x, auc_values, width, label='AUC')\n",
    "plt.bar([i + width for i in x], ndcg_values, width, label='nDCG@10')\n",
    "\n",
    "plt.xlabel('Time Splitting Strategy')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Performance Comparison of Time Splitting Strategies')\n",
    "plt.xticks([i + width/2 for i in x], list(split_configs.keys()))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}