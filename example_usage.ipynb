{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lavka Recommender System Example Usage\n\nThis notebook demonstrates how to use the recommender system with the unified experiment interface. It includes examples of new features like GPU acceleration, conversion-related features, and improved feature selection.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lavka_recsys.config import Config\n",
    "from lavka_recsys.experiment import Experiment, ExperimentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration\n",
    "\n",
    "You can either load configuration from a YAML file or create it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load from file if it exists\nif os.path.exists('default_config.yaml'):\n    config = Config.from_file('default_config.yaml')\nelse:\n    # Create configuration programmatically\n    config = Config({\n        \"experiment\": {\n            \"type\": \"standard\",\n            \"use_hyperparameter_tuning\": False,\n            \"evaluation\": {\n                \"perform_kaggle_simulation\": True,\n                \"create_submission\": True\n            }\n        },\n        \"model\": {\n            \"type\": \"catboost\",\n            \"use_gpu\": False,         # Set to True to enable GPU acceleration\n            \"gpu_devices\": \"0\",       # Specify GPU device ids (comma-separated for multiple GPUs)\n            \"thread_count\": -1,       # Number of CPU threads (-1 for auto)\n            \"config\": {\n                \"catboost\": {\n                    \"iterations\": 300,\n                    \"learning_rate\": 0.1,\n                    \"depth\": 6,\n                    \"l2_leaf_reg\": 3.0\n                }\n            }\n        },\n        \"feature_selection\": {\n            \"enabled\": False,         # Enable/disable feature selection\n            \"method\": \"importance\",   # Feature selection method\n            \"n_features\": 10          # Number of top features to select\n        },\n        \"features\": [\n            \"count_purchase_user_product\",\n            \"count_purchase_user_store\",\n            \"ctr_product\",\n            \"cart_to_purchase_rate\",  # New cart-to-purchase conversion rate\n            \"purchase_view_ratio\",    # New purchase-view ratio\n            \"recency_user_product\",\n            \"user_stats\",\n            \"product_stats\",\n            \"store_stats\"\n        ],\n        \"target\": \"CartUpdate_Purchase_vs_View\",\n        \"data\": {\n            \"train_path\": \"data/train.parquet\",\n            \"test_path\": \"data/test.parquet\",\n            \"sample_size\": 50000  # Use a smaller dataset for faster execution\n        },\n        \"training\": {\n            \"split_type\": \"standard\",  # Using standard split for quick demonstration\n            \"target_days\": 7,\n            \"validation_days\": 7\n        }\n    })"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Setup Experiment\n",
    "\n",
    "Create an experiment and set it up by loading the data and initializing components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "experiment = Experiment(\"example_experiment\", config)\n",
    "\n",
    "# Setup experiment (load data, initialize components)\n",
    "experiment.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Standard Experiment\n",
    "\n",
    "Run a standard experiment with a single train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment type to STANDARD\n",
    "experiment.experiment_type = ExperimentType.STANDARD\n",
    "\n",
    "# Run experiment\n",
    "results = experiment.run()\n",
    "\n",
    "# Print metrics\n",
    "print(\"Standard Experiment Metrics:\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "top_features = sorted(results['feature_importance'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for feature, importance in top_features:\n",
    "    print(f\"  {feature}: {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full History Experiment\n",
    "\n",
    "Run a full history experiment with time-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment type to FULL_HISTORY\n",
    "experiment.experiment_type = ExperimentType.FULL_HISTORY\n",
    "\n",
    "# Run experiment (this will take longer)\n",
    "results = experiment.run()\n",
    "\n",
    "# Print metrics\n",
    "print(\"Full History Experiment Metrics:\")\n",
    "for metric, value in results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "top_features = sorted(results['feature_importance'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for feature, importance in top_features:\n",
    "    print(f\"  {feature}: {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kaggle Evaluation and Submission\n",
    "\n",
    "Evaluate the model on simulated Kaggle test set and create a submission."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Update configuration for GPU and feature selection\ngpu_config = config.copy()\ngpu_config.set('model.use_gpu', True)  # Enable GPU\ngpu_config.set('feature_selection.enabled', True)  # Enable feature selection\ngpu_config.set('feature_selection.n_features', 8)  # Select top 8 features\n\n# Create and setup new experiment\nprint(\"Running experiment with GPU acceleration and feature selection...\")\ngpu_experiment = Experiment(\"gpu_feature_selection_example\", gpu_config)\ngpu_experiment.setup()\n\n# Run the experiment (this will be faster with GPU if available)\ngpu_results = gpu_experiment.run()\n\n# Print metrics\nprint(\"\\nGPU-accelerated Experiment Metrics:\")\nfor metric, value in gpu_results['metrics'].items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Print selected features\nif 'selected_features' in gpu_results:\n    print(\"\\nSelected Features:\")\n    for feature in gpu_results['selected_features']:\n        print(f\"  {feature}\")\n\n# Print feature importance for selected features\nif 'feature_importance' in gpu_results:\n    print(\"\\nFeature Importance:\")\n    # Filter to show only selected features\n    selected_features = set(gpu_results.get('selected_features', []))\n    filtered_importances = {k: v for k, v in gpu_results['feature_importance'].items() \n                           if not selected_features or k in selected_features}\n    \n    sorted_features = sorted(filtered_importances.items(), key=lambda x: x[1], reverse=True)\n    for feature, importance in sorted_features:\n        print(f\"  {feature}: {importance:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning Example\n",
    "\n",
    "Run an experiment with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration for tuning\n",
    "config.set('experiment.type', 'tuning')\n",
    "config.set('experiment.use_hyperparameter_tuning', True)\n",
    "config.set('hyperparameter_tuning.n_trials', 5)  # Low number for demonstration\n",
    "\n",
    "# Create and setup new experiment\n",
    "tuning_experiment = Experiment(\"tuning_example\", config)\n",
    "tuning_experiment.setup()\n",
    "\n",
    "# Run tuning (this will take longer)\n",
    "tuning_results = tuning_experiment.run()\n",
    "\n",
    "# Print results\n",
    "print(\"Tuning Experiment Metrics:\")\n",
    "for metric, value in tuning_results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in tuning_results['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 9. Examining Conversion Features\n\nLet's focus on the new conversion features: `cart_to_purchase_rate` and `purchase_view_ratio`.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create a configuration that only uses conversion features\nconversion_config = config.copy()\nconversion_config.set('features', [\n    \"cart_to_purchase_rate\",\n    \"purchase_view_ratio\",\n    \"ctr_product\",  # Include this for comparison\n    \"source_type\"   # Include a categorical feature\n])\n\n# Create and run experiment\nconversion_experiment = Experiment(\"conversion_features\", conversion_config)\nconversion_experiment.setup()\nconversion_results = conversion_experiment.run()\n\n# Print metrics\nprint(\"Conversion Features Experiment Metrics:\")\nfor metric, value in conversion_results['metrics'].items():\n    print(f\"  {metric}: {value:.4f}\")\n\n# Print feature importance\nif 'feature_importance' in conversion_results:\n    print(\"\\nFeature Importance:\")\n    sorted_features = sorted(conversion_results['feature_importance'].items(), \n                            key=lambda x: x[1], reverse=True)\n    for feature, importance in sorted_features:\n        print(f\"  {feature}: {importance:.6f}\")\n\n# Try to visualize feature distributions if data is available\nprint(\"\\nAttempting to visualize feature distributions...\")\ntry:\n    # Get a sample of the data with features\n    features_df = conversion_experiment.last_features\n    \n    if features_df is not None and not features_df.is_empty():\n        # Convert to pandas for easier plotting\n        pd_df = features_df.to_pandas()\n        \n        # Create plots for conversion features\n        plt.figure(figsize=(15, 5))\n        \n        # Plot 1: Cart-to-Purchase Rate\n        plt.subplot(1, 3, 1)\n        if 'cart_to_purchase_rate' in pd_df.columns:\n            plt.hist(pd_df['cart_to_purchase_rate'].dropna(), bins=20, alpha=0.7)\n            plt.title('Cart-to-Purchase Rate')\n            plt.xlabel('Rate')\n            plt.ylabel('Count')\n        else:\n            plt.title('Cart-to-Purchase Rate (Not Available)')\n        \n        # Plot 2: Purchase-View Ratio\n        plt.subplot(1, 3, 2)\n        if 'purchase_view_ratio' in pd_df.columns:\n            plt.hist(pd_df['purchase_view_ratio'].dropna(), bins=20, alpha=0.7)\n            plt.title('Purchase-View Ratio')\n            plt.xlabel('Ratio')\n            plt.ylabel('Count')\n        else:\n            plt.title('Purchase-View Ratio (Not Available)')\n        \n        # Plot 3: CTR Product (for comparison)\n        plt.subplot(1, 3, 3)\n        if 'ctr_product' in pd_df.columns:\n            plt.hist(pd_df['ctr_product'].dropna(), bins=20, alpha=0.7)\n            plt.title('Product CTR')\n            plt.xlabel('CTR')\n            plt.ylabel('Count')\n        else:\n            plt.title('Product CTR (Not Available)')\n        \n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"No feature data available for visualization\")\nexcept Exception as e:\n    print(f\"Could not visualize features: {str(e)}\")\n    print(\"This is expected if using a cached model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Comparison\n",
    "\n",
    "Compare metrics across different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart for AUC comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "auc_values = [metrics[split].get('auc', 0) for split in split_configs.keys()]\n",
    "ndcg_values = [metrics[split].get('ndcg@10', 0) for split in split_configs.keys()]\n",
    "\n",
    "x = range(len(split_configs))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x, auc_values, width, label='AUC')\n",
    "plt.bar([i + width for i in x], ndcg_values, width, label='nDCG@10')\n",
    "\n",
    "plt.xlabel('Time Splitting Strategy')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Performance Comparison of Time Splitting Strategies')\n",
    "plt.xticks([i + width/2 for i in x], list(split_configs.keys()))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}